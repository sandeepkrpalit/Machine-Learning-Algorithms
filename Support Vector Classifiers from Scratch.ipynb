{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifiers (SVCs)\n",
    "\n",
    "Support Vector Classifiers are a type of machine learning algorithm that can be used for classification tasks. They are based on the idea of finding a hyperplane that separates the data into two classes with the largest possible margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How SVCs work\n",
    "\n",
    "Support vector Machines (SVMs) function by identifying the hyperplane that maximizes the margin between two classes. This margin represents the distance between the hyperplane and the nearest data points from each class. \n",
    "\n",
    "SVM employ quadratic programming, a mathematical technique designed for optimizing problems with quadratic objective functions and linear constraints, to detetermine the hyperplane that maximizes the margin between two classes. The margin is computed using the following:\n",
    "\n",
    "                    `[m = 2/||w||]`\n",
    "\n",
    "Here, 'w' signifies the weight vector, measuring the distance between the hyperplane and closest data points belonging to each class.\n",
    "\n",
    "Once the hyperplane is computed, SVMs are capable of classifying new data points by predicting the class of the data point closest to the hyperplane. In the case of linear SVMs, the predictive function is expressed as:\n",
    "\n",
    "                    `[f(x) = w^T * x + b]`\n",
    "\n",
    "Here, 'f(x)' represents the predicted class label, 'x' is the input feature vector, 'w' denotes the weight vector, and 'b' denotes the bias term.\n",
    "\n",
    "The weight vector 'w' and bias term 'b' are learned through the training process. 'w' indicates the orientation of the hyperplane that separates the two classes, while 'b' indicates the hyperplane's displacement from the origin.\n",
    "\n",
    "For predictions, the linear SVM calculates the dot product of the input feature vector 'x' and the weight vector 'w'. If the result is positive, the predicted class label is 1; if negative, it is 0.\n",
    "\n",
    "#### Hinge Loss Function:\n",
    "\n",
    "                    `[L(y, f(x)) = max(0, 1 - yf(x))]`\n",
    "\n",
    "where: `y` denotes the true label (+1 or -1).\n",
    "        `f(x)` signifies the predicted label (sgn(w^T * x + b))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of SVCs\n",
    "\n",
    "There are mainly two main types of SVCs: linear SVCs and nonlinear SVCs.\n",
    "\n",
    "#### Linear SVC:\n",
    "It find a linear hyperplane that separates the two classes. Linear SVCs are computationally efficient and easy to interpret. However, they are not as flexible as nonlinear SVCs and may not be able to learn complex relationships between the input features.\n",
    "\n",
    "#### Nonlinear SVCs\n",
    "Nonlinear SVCs fina a nonlinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
