{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that is widely used in machine learning. It transforms a set of correlated variables into a set of uncorrelated variables, called principal components. PCA is often used to reduce the number of features in a dataset without losing too much information. This can make machine learning algorithms more efficient and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How PCA works?\n",
    "\n",
    "PCA works by finding the direction of greatest variance in the data. These directions are the principal components. The first principal component is the direction of the greatest variance, the second principal component is the direction of the second greatest variance, and so on.\n",
    "\n",
    "#### Eigendecomposition\n",
    "To calculate the principal components of a dataset, PCA uses a technique called Eigendecomposition. Eigendecomposition is a mathematical technique that decomposes a matrix into its eigenvectors and eigenvalues. The eigenvectors of a matrix represent the directions of greatest variance in the matrix, and the eigenvalues of a matrix represent the amount of variance in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## PCA Formula\n",
    "\n",
    "The PCA formula can be written as follows:\n",
    "\n",
    "PCA(X) = W * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "1. `PCA(X)` is the transformed data\n",
    "2. `X` is the original data\n",
    "3. `W` is the projection matrix, which contains the eigenvectors of the covariance matrix of X.\n",
    "\n",
    "The projection matrix `W` is calculated by peforming eigendecomposition on the covariance matrix of `X` and selecting the top n_components eigenvectors, where `n_components` is the number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of using PCA\n",
    "\n",
    "1. Dimensionality reduction\n",
    "2. Noise reduction\n",
    "3. Feature selection\n",
    "   \n",
    "### Pitfalls of PCA\n",
    "\n",
    "1. PCA is sensitive to outliers.\n",
    "2. PCA can remove important information from the data.\n",
    "3. PCA is not invariant to nonlinear transformations.\n",
    "4. PCA can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation from Scratch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.eigenvalues = None\n",
    "\n",
    "    def fit(self, X):\n",
    "    # Center the data\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
