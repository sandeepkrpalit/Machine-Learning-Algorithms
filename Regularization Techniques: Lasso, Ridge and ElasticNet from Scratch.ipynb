{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used in machine learning to reduce overfitting. Overfitting occurs when a model learns the training data too well and is unable to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly three types of regularization: `Lasso`, `Ridge` and `Elastic Net`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "lasso regression, also known as Least Absolute Shrinkage and Selection Operator, is a type of regularization that penalizes the L1 norm of the coefficients. The L1 norm is the sum of the absolute values of the coefficients. This penalization forces the model to shrink the coefficients towards zero, which can lead to some coefficients towards zero, which can lead to some coefficients being set to zero exaactly. This feature selection capability of Lasso regression makes it a powerful tool for variable selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python, Lasso formula:\n",
    "\n",
    "Lasso(B) = sum((vi - x.T*B)^2) + lambda * sum(abs(Bj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "1. `B` is the vector of coefficients.\n",
    "2. `vi` is the target variable for the i-th observation.\n",
    "3. `x.T*B` is the predicted target variable for the i-th observation, given the coefficients vector B.\n",
    "4. `lambda` is the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression penalizes the L2 norm of the coefficients. The L2 norm is the square root of the sum of the squared coefficients. This penalization shrinks the coefficients towards zero, but it does not set any coefficients to zero exactly. This makes Ridge regression a more robust regularization technique than Lasso regression, but it is not as effective for variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python, Ridge formula:\n",
    "\n",
    "Lasso(B) = sum((vi - x.T*B)^2) + lambda * sum(Bj**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "1. `B` is the vector of coefficients.\n",
    "2. `vi` is the target variable for the i-th observation.\n",
    "3. `x.T*B` is the predicted target variable for the i-th observation, given the coefficients vector B.\n",
    "4. `lambda` is the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge formula is similar to the Lasso formula, except that the regularization penalty is the L2 norm of the coefficients, rather than the L1 norm. This means that the Ridge model will shrink the coefficients towards zero, but it will not set any coefficients to zero exactly.\n",
    "\n",
    "Ridge regularization is weaker for variable selection than the Lasso model, as it's coefficients are shrunk to 0, but it is more robust to outliers and noise. This makes it a good choice for machine learning problems where robustness is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "Elastic Net is a hybrid regularization technique that combines the L1 and L2 penalties. The Elastic Net penalty is a weighted sum of the L1 and L2 penalties, where the weight parameter controls the relative importance of the two peanlties. Elastic Net provides a balance between the variable selection capability of Lasso regression and the robustness of Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python, the Elastic Net formula\n",
    "\n",
    "ElasticNet(B) = sum((vi - x.T*B)^2) + lambda * (alpha * sum(abs(Bj)) + 0.5 * (1 - alpha) * sum(Bj**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "1. `B` is the vector of coefficients.\n",
    "2. `vi` is the target variable for the i-th observation.\n",
    "3. `x.T*B` is the predicted target variable for the i-th observation, given the coefficients vector B.\n",
    "4. `lambda` is the regularization parameter.\n",
    "5. `alpha` is the weight parameter for the L1 and L2 penalties. A larger value of alpha will lead to a more sparse model, where only a few coefficients will be non-zero. A smaller value of alpha will lead to a more dense model, where all coefficients will be non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
